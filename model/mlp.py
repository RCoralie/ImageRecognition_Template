#!/usr/bin/env python2 
# -*- coding: utf-8 -*- 
"""
    A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)

    In addition, a couple of small features :
    Visualization of the model with TensorBoard: TensorBoard is included with TensorFlow and allows you to generate charts and graphs from your models and from data generated by your models. This helps with analyzing your models and is especially useful for debugging. [see : https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard]
    Checkpoints: this feature allows you to save the current state of your model for later use. Training a model can take quite a while, so itâ€™s essential to not have to start from scratch each time you want to use it.
"""

import os
import time
import logging
import tensorflow as tf

from datetime import datetime

from ..util.dataset_reader import DataRead

tf.logging.set_verbosity(tf.logging.ERROR)


class MultilayerPerceptron(object):

    def __init__(self,
                 config,
                 batch_size,
                 initial_learning_rate,
                 steps_per_checkpoint,
                 max_checkpoints,
                 model_dir):

        # Global Parameters
        self.config = config
        self.learning_rate = initial_learning_rate
        self.batch_size = batch_size
        self.model_dir = model_dir
        self.steps_per_checkpoint = steps_per_checkpoint

        # DEFINES THE MODEL --------------------------------------------------------------------------------

        # Network Parameters
        self.n_input = self.config.IMAGE_HEIGHT * self.config.IMAGE_WIDTH # size of the data input (total pixels)
        self.n_hidden_1 = (self.n_input//2) # 1st layer number of neurons
        self.n_hidden_2 = (self.n_hidden_1//2) # 2nd layer number of neurons
        self.n_classes = len(self.config.CHARMAP) # total classes (0-9 digits)

        # Graph input
        self.X = tf.placeholder('float', [None, self.n_input], name='X')
        self.Y = tf.placeholder('float', [None, self.n_classes], name='Y')

        # Hidden fully connected layer
        with tf.name_scope('HiddenLayer1'):
            weights = tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1]), name='weights')
            biases = tf.Variable(tf.random_normal([self.n_hidden_1]), name='biases')
            layer_1 = tf.add(tf.matmul(self.X, weights), biases)
        # Hidden fully connected layer
        with tf.name_scope('HiddenLayer2'):
            weights = tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2]), name='weights')
            biases = tf.Variable(tf.random_normal([self.n_hidden_2]), name='biases')
            layer_2 = tf.add(tf.matmul(layer_1, weights), biases)
        # Output fully connected layer with a neuron for each class
        with tf.name_scope('OutputLayer'):
            weights = tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes]), name='weights')
            biases = tf.Variable(tf.random_normal([self.n_classes]), name='biases')
            y_ = tf.matmul(layer_2, weights) + biases
            tf.summary.histogram('logits', y_) # summary-operation for 'logits' variable

        # DEFINES OPERATION (TO TRAIN & EVALUATE) ----------------------------------------------------------

        # Define operation for calculates the loss from logits and labels
        with tf.name_scope('Loss'):
            self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=self.Y, name='cross_entropy'))
            tf.summary.scalar('loss', self.loss_op) # scalar summary for the loss

        # Define the training operation with optimizer (which also increments the global step counter)
        with tf.name_scope('TrainingStep'):
            self.global_step = tf.Variable(0, name='global_step', trainable=False)
            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
            self.train_op = self.optimizer.minimize(self.loss_op, global_step=self.global_step)

        # Define operation calculating the accuracy
        with tf.name_scope('Accuracy'):
            correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(self.Y,1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            tf.summary.scalar('train_accuracy', self.accuracy) # scalar summary for the accuracy

        # INITIALIZATION -----------------------------------------------------------------------------------

        self.init = tf.global_variables_initializer()

        # VISUALIZATION & CHECKPOINTS ----------------------------------------------------------------------

        # Define checkpoints saver
        self.checkpoint_path = os.path.join(self.model_dir, "model.ckpt")
        self.saver = tf.train.Saver(max_to_keep=max_checkpoints)

        # Define summaries repertory & create a single operation which runs all summaries
        now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
        self.logdir = "{}/run-{}/".format(self.model_dir, now)
        self.summary = tf.summary.merge_all()


    def train(self, dataset_path, num_epoch, load_model, log_step):

        logging.info('num_epoch: %d' % num_epoch)
        step_time = 0.0
        loss = 0.0
        current_step = 0

        logging.info('Starting the training process.')
        s_gen = DataRead(self.config, dataset_path, epochs=num_epoch)
        with tf.Session() as sess:
            # Initialize or load model parameters & create summary-writer
            ckpt = tf.train.get_checkpoint_state(self.model_dir)
            if ckpt and load_model:
                logging.info('Reading model parameters from %s' % ckpt.model_checkpoint_path)
                self.saver.restore(sess, ckpt.model_checkpoint_path)
            else:
                logging.info('Initialize model parameters.')
                sess.run(self.init)
            summary_writer = tf.summary.FileWriter(self.logdir, sess.graph)
            # Gradual improvement of model parameters during training steps
            for batch in s_gen.gen(self.batch_size):
                current_step += 1
                global_step = sess.run(self.global_step)
                start_time = time.time()
                batch_x = batch['data']
                batch_y = batch['labels']
                feed_dict = {self.X: batch_x, self.Y: batch_y}
                logging.info('Shape of batch X : ' + str(tuple(batch_x.shape)))
                logging.info('Shape of batch Y : ' + str(tuple(batch_y.shape)))
                _, loss_value = sess.run([self.train_op, self.loss_op], feed_dict)
                curr_step_time = (time.time() - start_time)

                # Print statistics for the previous epoch.
                logging.info('Step %04d - Time: %.3f, loss: %.4f' % (current_step, curr_step_time, loss_value))

                # Periodically save checkpoint & summary
                if current_step % self.steps_per_checkpoint == 0:
                    logging.info('Saving model parameters at global step %d.' % global_step)
                    save_path = self.saver.save(sess, self.checkpoint_path, global_step=global_step)
                    summary_str = sess.run(self.summary, feed_dict=feed_dict)
                    summary_writer.add_summary(summary_str, global_step)

        logging.info('Training Finished!')
        save_path = self.saver.save(sess, self.checkpoint_path, global_step=global_step)
        logging.info('Model stored.')
        writer.close()

